<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>project.fawkes.utils.utils API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>project.fawkes.utils.utils</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import json
import ast
import sys
import os
import re
import csv
import itertools
import operator
import dateutil.parser
import hashlib

import gensim
import spacy
import jsonschema

from pprint import pprint
from datetime import datetime, timedelta

from gensim.utils import simple_preprocess
from nltk.corpus import stopwords

#  This is so that the following imports work
sys.path.append(os.path.realpath(&#34;.&#34;))

import fawkes.constants as constants
from fawkes.stop_words_file import EXTENDED_STOP_WORDS

def open_json(file_location):
    with open(file_location, &#34;r&#34;) as read_file:
        documents = json.load(read_file)
    return documents


def dump_json(records, write_file):
    with open(write_file, &#34;w&#34;) as file:
        json.dump(records, file, indent=4)

def get_json_key_value(json_object, keys_list):
    &#34;&#34;&#34; Get the value from json pointing to string of keys input: [k1,k2] &#34;&#34;&#34;
    # This will be the key
    key = keys_list[0]

    # We check for types, if its a dict or list
    if isinstance(json_object, list):
        key = int(key)
        # Check for index out of range
        if key &gt;= len(json_object):
            return None

    if len(keys_list) == 1:
        # We first check if its a list and then move forward.
        # Think, why we did it like that ?
        if isinstance(json_object, list):
            return json_object[key]
        elif key in json_object:
            return json_object[key]
        else:
            return None

    return get_json_key_value(json_object[key], keys_list[1:])

def check_tweet_authenticity(tweet_message, twitter_handle_blacklist):
    &#34;&#34;&#34;  Checks if tweets incoming are authentic. basically there is blacklist of twitter-handles &#34;&#34;&#34;
    is_tweet_authentic = True

    for handle in twitter_handle_blacklist:
        if handle in tweet_message:
            is_tweet_authentic = False

    return is_tweet_authentic


def check_for_explicit_content(tweet):
    if constants.POSSIBLY_SENSITIVE in tweet:
        return tweet[constants.POSSIBLY_SENSITIVE]
    return True


def tokenise(document):
    return gensim.utils.simple_preprocess(str(document), deacc=True)


def remove_stop_words(document):
    &#34;&#34;&#34;
        Removes stop words. Takes tokenised document as input and returns
        after removing the stop words.
        input : [&#34;phil&#34;,&#34;is&#34;,&#34;good&#34;]
        output : [&#34;phil&#34;,&#34;good&#34;]
    &#34;&#34;&#34;

    stop_words = stopwords.words(&#34;english&#34;)
    stop_words = set(stop_words + EXTENDED_STOP_WORDS)
    return [token for token in document if token not in stop_words]


def lemmatisation(text, allowed_postags=[&#34;NOUN&#34;, &#34;ADJ&#34;, &#34;VERB&#34;, &#34;ADV&#34;]):
    &#34;&#34;&#34;
    Does lemmatisation. whats lemmatisation? google :P
    Input : [&#34;phil&#34;, &#34;is&#34;, &#34;good&#34;]
    - https://spacy.io/api/annotation
    - https://spacy.io/api/top-level
    &#34;&#34;&#34;
    nlp = spacy.load(&#34;en&#34;, disable=[&#34;parser&#34;, &#34;ner&#34;])
    doc = nlp(&#34; &#34;.join(text))
    return [token.lemma_ for token in doc if token.pos_ in allowed_postags]

def calculate_hash(string):
    return hashlib.sha1(string.encode(&#34;utf-8&#34;)).hexdigest()

def most_common(L):
    # https://stackoverflow.com/a/1520716/3751615
    # get an iterable of (item, iterable) pairs
    SL = sorted((x, i) for i, x in enumerate(L))
    groups = itertools.groupby(SL, key=operator.itemgetter(0))

    # auxiliary function to get &#34;quality&#34; for an item

    def _auxfun(g):
        item, iterable = g
        count = 0
        min_index = len(L)
        for _, where in iterable:
            count += 1
            min_index = min(min_index, where)
            # print &#39;item %r, count %r, minind %r&#39; % (item, count, min_index)
        return count, -min_index

    # pick the highest-count/earliest item
    return max(groups, key=_auxfun)[0]


def get_sentiment_compound(review):
    return review.derived_insight.sentiment[&#34;compound&#34;]


def fetch_channel_config(app_config, channel_type):
    for review_channel in app_config.review_channels:
        if review_channel.channel_type == channel_type:
            return review_channel
    return None</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="project.fawkes.utils.utils.calculate_hash"><code class="name flex">
<span>def <span class="ident">calculate_hash</span></span>(<span>string)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_hash(string):
    return hashlib.sha1(string.encode(&#34;utf-8&#34;)).hexdigest()</code></pre>
</details>
</dd>
<dt id="project.fawkes.utils.utils.check_for_explicit_content"><code class="name flex">
<span>def <span class="ident">check_for_explicit_content</span></span>(<span>tweet)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_for_explicit_content(tweet):
    if constants.POSSIBLY_SENSITIVE in tweet:
        return tweet[constants.POSSIBLY_SENSITIVE]
    return True</code></pre>
</details>
</dd>
<dt id="project.fawkes.utils.utils.check_tweet_authenticity"><code class="name flex">
<span>def <span class="ident">check_tweet_authenticity</span></span>(<span>tweet_message, twitter_handle_blacklist)</span>
</code></dt>
<dd>
<div class="desc"><p>Checks if tweets incoming are authentic. basically there is blacklist of twitter-handles</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_tweet_authenticity(tweet_message, twitter_handle_blacklist):
    &#34;&#34;&#34;  Checks if tweets incoming are authentic. basically there is blacklist of twitter-handles &#34;&#34;&#34;
    is_tweet_authentic = True

    for handle in twitter_handle_blacklist:
        if handle in tweet_message:
            is_tweet_authentic = False

    return is_tweet_authentic</code></pre>
</details>
</dd>
<dt id="project.fawkes.utils.utils.dump_json"><code class="name flex">
<span>def <span class="ident">dump_json</span></span>(<span>records, write_file)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dump_json(records, write_file):
    with open(write_file, &#34;w&#34;) as file:
        json.dump(records, file, indent=4)</code></pre>
</details>
</dd>
<dt id="project.fawkes.utils.utils.fetch_channel_config"><code class="name flex">
<span>def <span class="ident">fetch_channel_config</span></span>(<span>app_config, channel_type)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fetch_channel_config(app_config, channel_type):
    for review_channel in app_config.review_channels:
        if review_channel.channel_type == channel_type:
            return review_channel
    return None</code></pre>
</details>
</dd>
<dt id="project.fawkes.utils.utils.get_json_key_value"><code class="name flex">
<span>def <span class="ident">get_json_key_value</span></span>(<span>json_object, keys_list)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the value from json pointing to string of keys input: [k1,k2]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_json_key_value(json_object, keys_list):
    &#34;&#34;&#34; Get the value from json pointing to string of keys input: [k1,k2] &#34;&#34;&#34;
    # This will be the key
    key = keys_list[0]

    # We check for types, if its a dict or list
    if isinstance(json_object, list):
        key = int(key)
        # Check for index out of range
        if key &gt;= len(json_object):
            return None

    if len(keys_list) == 1:
        # We first check if its a list and then move forward.
        # Think, why we did it like that ?
        if isinstance(json_object, list):
            return json_object[key]
        elif key in json_object:
            return json_object[key]
        else:
            return None

    return get_json_key_value(json_object[key], keys_list[1:])</code></pre>
</details>
</dd>
<dt id="project.fawkes.utils.utils.get_sentiment_compound"><code class="name flex">
<span>def <span class="ident">get_sentiment_compound</span></span>(<span>review)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_sentiment_compound(review):
    return review.derived_insight.sentiment[&#34;compound&#34;]</code></pre>
</details>
</dd>
<dt id="project.fawkes.utils.utils.lemmatisation"><code class="name flex">
<span>def <span class="ident">lemmatisation</span></span>(<span>text, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])</span>
</code></dt>
<dd>
<div class="desc"><p>Does lemmatisation. whats lemmatisation? google :P
Input : ["phil", "is", "good"]
- <a href="https://spacy.io/api/annotation">https://spacy.io/api/annotation</a>
- <a href="https://spacy.io/api/top-level">https://spacy.io/api/top-level</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def lemmatisation(text, allowed_postags=[&#34;NOUN&#34;, &#34;ADJ&#34;, &#34;VERB&#34;, &#34;ADV&#34;]):
    &#34;&#34;&#34;
    Does lemmatisation. whats lemmatisation? google :P
    Input : [&#34;phil&#34;, &#34;is&#34;, &#34;good&#34;]
    - https://spacy.io/api/annotation
    - https://spacy.io/api/top-level
    &#34;&#34;&#34;
    nlp = spacy.load(&#34;en&#34;, disable=[&#34;parser&#34;, &#34;ner&#34;])
    doc = nlp(&#34; &#34;.join(text))
    return [token.lemma_ for token in doc if token.pos_ in allowed_postags]</code></pre>
</details>
</dd>
<dt id="project.fawkes.utils.utils.most_common"><code class="name flex">
<span>def <span class="ident">most_common</span></span>(<span>L)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def most_common(L):
    # https://stackoverflow.com/a/1520716/3751615
    # get an iterable of (item, iterable) pairs
    SL = sorted((x, i) for i, x in enumerate(L))
    groups = itertools.groupby(SL, key=operator.itemgetter(0))

    # auxiliary function to get &#34;quality&#34; for an item

    def _auxfun(g):
        item, iterable = g
        count = 0
        min_index = len(L)
        for _, where in iterable:
            count += 1
            min_index = min(min_index, where)
            # print &#39;item %r, count %r, minind %r&#39; % (item, count, min_index)
        return count, -min_index

    # pick the highest-count/earliest item
    return max(groups, key=_auxfun)[0]</code></pre>
</details>
</dd>
<dt id="project.fawkes.utils.utils.open_json"><code class="name flex">
<span>def <span class="ident">open_json</span></span>(<span>file_location)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def open_json(file_location):
    with open(file_location, &#34;r&#34;) as read_file:
        documents = json.load(read_file)
    return documents</code></pre>
</details>
</dd>
<dt id="project.fawkes.utils.utils.remove_stop_words"><code class="name flex">
<span>def <span class="ident">remove_stop_words</span></span>(<span>document)</span>
</code></dt>
<dd>
<div class="desc"><p>Removes stop words. Takes tokenised document as input and returns
after removing the stop words.
input : ["phil","is","good"]
output : ["phil","good"]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_stop_words(document):
    &#34;&#34;&#34;
        Removes stop words. Takes tokenised document as input and returns
        after removing the stop words.
        input : [&#34;phil&#34;,&#34;is&#34;,&#34;good&#34;]
        output : [&#34;phil&#34;,&#34;good&#34;]
    &#34;&#34;&#34;

    stop_words = stopwords.words(&#34;english&#34;)
    stop_words = set(stop_words + EXTENDED_STOP_WORDS)
    return [token for token in document if token not in stop_words]</code></pre>
</details>
</dd>
<dt id="project.fawkes.utils.utils.tokenise"><code class="name flex">
<span>def <span class="ident">tokenise</span></span>(<span>document)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tokenise(document):
    return gensim.utils.simple_preprocess(str(document), deacc=True)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="project.fawkes.utils" href="index.html">project.fawkes.utils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="project.fawkes.utils.utils.calculate_hash" href="#project.fawkes.utils.utils.calculate_hash">calculate_hash</a></code></li>
<li><code><a title="project.fawkes.utils.utils.check_for_explicit_content" href="#project.fawkes.utils.utils.check_for_explicit_content">check_for_explicit_content</a></code></li>
<li><code><a title="project.fawkes.utils.utils.check_tweet_authenticity" href="#project.fawkes.utils.utils.check_tweet_authenticity">check_tweet_authenticity</a></code></li>
<li><code><a title="project.fawkes.utils.utils.dump_json" href="#project.fawkes.utils.utils.dump_json">dump_json</a></code></li>
<li><code><a title="project.fawkes.utils.utils.fetch_channel_config" href="#project.fawkes.utils.utils.fetch_channel_config">fetch_channel_config</a></code></li>
<li><code><a title="project.fawkes.utils.utils.get_json_key_value" href="#project.fawkes.utils.utils.get_json_key_value">get_json_key_value</a></code></li>
<li><code><a title="project.fawkes.utils.utils.get_sentiment_compound" href="#project.fawkes.utils.utils.get_sentiment_compound">get_sentiment_compound</a></code></li>
<li><code><a title="project.fawkes.utils.utils.lemmatisation" href="#project.fawkes.utils.utils.lemmatisation">lemmatisation</a></code></li>
<li><code><a title="project.fawkes.utils.utils.most_common" href="#project.fawkes.utils.utils.most_common">most_common</a></code></li>
<li><code><a title="project.fawkes.utils.utils.open_json" href="#project.fawkes.utils.utils.open_json">open_json</a></code></li>
<li><code><a title="project.fawkes.utils.utils.remove_stop_words" href="#project.fawkes.utils.utils.remove_stop_words">remove_stop_words</a></code></li>
<li><code><a title="project.fawkes.utils.utils.tokenise" href="#project.fawkes.utils.utils.tokenise">tokenise</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>